# Repo2Txt

**AI-Powered Repository Analysis & LLM-Ready Text Conversion**

Transform GitHub repositories and local codebases into LLM-friendly formats with intelligent file selection, comprehensive token analysis, and conversational AI assistance.

## üåü What's New

### ü§ñ AI-Powered File Selection
- **Conversational Interface**: Chat with an AI assistant to select files naturally
- **Smart Recommendations**: AI understands your intent and suggests relevant files
- **Multiple Prompt Styles**: Choose between standard or meta-reasoning prompts
- **Token Budget Management**: AI respects your token limits and optimises selections
- **Interactive Commands**: Full command system for fine-tuning your selection

### üèóÔ∏è Modern Architecture
- **Modular Design**: Clean, testable components built for extensibility
- **Robust Error Handling**: Graceful fallbacks and survival-oriented design
- **Performance Optimized**: Token caching and efficient processing
- **Developer Friendly**: Rich debugging tools and comprehensive logging

## üöÄ Key Features

### Dual Analysis Modes

**üéØ AI-Assisted Selection** (*Recommended*)
- Natural language queries for file selection
- Intelligent recommendations based on your goals
- Interactive chat interface with command system
- Real-time token budget tracking
- Multiple output formats (Markdown, XML, JSON)

**üìÇ Manual Selection** 
- Interactive directory navigation
- Granular file/folder selection
- Back navigation and flexible options
- Visual progress tracking

### Advanced Capabilities
- **Multi-Source Support**: GitHub repositories, local directories, and archives
- **Intelligent Processing**: Binary detection, encoding fallbacks, size limits
- **Rich Output Formats**: Markdown, XML, or JSON with detailed token reports
- **Token Analysis**: Comprehensive counting, distribution stats, and budget recommendations
- **Theme Support**: Multiple terminal color themes (Manhattan, Matrix, Green, Sunset)

## üìã Prerequisites

- **Python 3.8+** (recommended: 3.11+)
- **LLM API Access**: OpenAI, Ollama, llama.cpp, or compatible endpoints
- **GitHub Token**: For private repositories (optional)

## üîß Installation

### Quick Start with uv (Recommended)
```bash
git clone https://github.com/your-username/repo2txt.git
cd repo2txt
uv sync
source .venv/bin/activate
```

### Traditional Installation
```bash
git clone https://github.com/your-username/repo2txt.git
cd repo2txt
pip install -e .
```

### Development Setup
```bash
uv sync --dev
source .venv/bin/activate
pytest  # Run tests
```

## üöÄ Quick Start

### 1. AI-Assisted Analysis (Recommended)

```bash
# Analyze current directory with AI assistance
repo2txt . --ai-select

# Use specific query for targeted selection  
repo2txt . --ai-select --ai-query "Show me the main API endpoints and database models"

# Advanced options
repo2txt . --ai-select --prompt-style meta-reasoning --token-budget 50000 --theme matrix
```

### 2. Traditional Manual Selection

```bash
# Analyze with interactive selection
repo2txt /path/to/repo

# GitHub repository
repo2txt https://github.com/owner/repo

# Export multiple formats
repo2txt . --format xml --json
```

## ü§ñ AI Chat Commands

During AI-assisted selection, use these commands:

### Core Commands
- `/help` - Show all available commands
- `/generate [format]` - Create output files (markdown, xml, json, all)
- `/save [filename]` - Save chat history
- `/clear` - Reset conversation and selection
- `/quit` - Exit the application

### Selection Control
- `/undo` - Undo last action
- `/redo` - Regenerate last AI response
- `/toggle streaming` - Enable/disable streaming responses
- `/toggle thinking` - Enable/disable thinking mode (Qwen models)
- `/toggle prompt` - Cycle through prompt styles
- `/toggle budget <N>` - Set token budget

### Debug & Development
- `/debug` - Toggle debug mode
- `/debug state` - Show current configuration

## üé® Configuration

### Environment Variables

Create a `.env` file:
```bash
# LLM Configuration
LLM_PROVIDER=openai          # openai, ollama, llamacpp
LLM_MODEL=gpt-4-turbo        # Model name
LLM_API_KEY=your_api_key     # API key
LLM_BASE_URL=                # Custom endpoint (optional)

# GitHub Access
GITHUB_TOKEN=your_token      # For private repos

# UI Preferences  
DEFAULT_THEME=manhattan      # manhattan, matrix, green, sunset
```

### Command Line Options

```bash
repo2txt [REPO] [OPTIONS]

Arguments:
  REPO                    Repository path, GitHub URL, or GitHub shorthand (owner/repo)

Core Options:
  --output-dir, -o DIR    Output directory (default: output)
  --format, -f FORMAT     Output format: xml, markdown (default: markdown)
  --theme, -t THEME       Terminal theme: manhattan, green, matrix, sunset
  --max-file-size SIZE    Maximum file size in bytes (default: 1MB)
  --no-tokens            Disable token counting
  --json                 Export token data as JSON

AI Selection Options:
  --ai-select            Enable AI-assisted file selection
  --ai-query QUERY       Specific query for AI selection
  --token-budget N       Token budget for AI selection (default: 100000)
  --prompt-style STYLE   Prompt style: standard, meta-reasoning, xml
  --debug               Enable debug mode (shows system prompts, tool calls)
```

## üìä Output Structure

### Default Output (2-3 files)
```
output/
‚îî‚îÄ‚îÄ RepoName_20240315_143022/
    ‚îú‚îÄ‚îÄ RepoName_analysis.md      # Main content in Markdown
    ‚îú‚îÄ‚îÄ RepoName_tokens.txt       # Detailed token report  
    ‚îî‚îÄ‚îÄ RepoName_tokens.json      # Token data (if --json)
```

### AI Chat History (optional)
```
output/
‚îî‚îÄ‚îÄ chat_history/
    ‚îî‚îÄ‚îÄ chat_history_20240315_143022.json
```

## üéØ Example Workflows

### 1. Quick API Documentation
```bash
repo2txt . --ai-select --ai-query "Select all API route files and documentation"
```

### 2. Architecture Overview
```bash
repo2txt . --ai-select --ai-query "Show me the main architecture components and configuration files"
```
### 3. Interactive Exploration
```bash
repo2txt . --ai-select --debug --theme matrix
# Then chat: "What are the main components of this project?"
```

## üèóÔ∏è Architecture

### Core Components

```
repo2txt/
‚îú‚îÄ‚îÄ adapters/           # Repository source adapters (GitHub, local, etc.)
‚îú‚îÄ‚îÄ ai/                # AI-powered file selection system
‚îÇ   ‚îú‚îÄ‚îÄ agent_session.py    # Session state management
‚îÇ   ‚îú‚îÄ‚îÄ chat_orchestrator.py # Chat flow coordination  
‚îÇ   ‚îú‚îÄ‚îÄ command_handler.py   # Command processing
‚îÇ   ‚îú‚îÄ‚îÄ file_selector_agent.py # Main AI agent
‚îÇ   ‚îú‚îÄ‚îÄ llm.py              # LLM client & streaming
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py          # System prompt generation
‚îÇ   ‚îú‚îÄ‚îÄ qwen_utils.py       # Qwen model utilities
‚îÇ   ‚îú‚îÄ‚îÄ state.py            # File selection state & token cache
‚îÇ   ‚îî‚îÄ‚îÄ tools.py            # AI function calling tools
‚îú‚îÄ‚îÄ core/              # Analysis engine
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py         # Main analysis orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ file_analyzer.py    # Individual file processing
‚îÇ   ‚îú‚îÄ‚îÄ models.py           # Core data structures
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer.py        # Token counting utilities
‚îî‚îÄ‚îÄ utils/             # Shared utilities
    ‚îú‚îÄ‚îÄ console.py          # Terminal UI management
    ‚îú‚îÄ‚îÄ console_base.py     # Base console functionality
    ‚îú‚îÄ‚îÄ encodings.py        # File encoding detection
    ‚îú‚îÄ‚îÄ file_filter.py      # File filtering logic
    ‚îî‚îÄ‚îÄ logging_config.py   # Logging configuration
```

### Design Principles

- **Modular Architecture**: Each component has a single, clear responsibility
- **Survival-Oriented Error Handling**: Graceful degradation when components fail
- **Performance by Default**: Token caching, efficient processing, minimal I/O
- **Developer Experience**: Rich debugging, comprehensive testing, clear APIs

## üß™ Testing

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_ai_components.py      # AI system tests
pytest tests/test_analyzer.py           # Core analysis tests
pytest tests/test_tokenizer.py          # Token counting tests

# Run with coverage
pytest --cov=repo2txt --cov-report=html
```

## üîß Advanced Usage

### Custom LLM Endpoints

```bash
# Ollama
export LLM_PROVIDER=ollama
export LLM_BASE_URL=http://localhost:11434
export LLM_MODEL=qwen3:32b

# llama.cpp server
export LLM_PROVIDER=llamacpp  
export LLM_BASE_URL=http://localhost:8080
export LLM_MODEL=llama3.2:3b
```

### Programmatic Usage

```python
from repo2txt.core.analyzer import RepositoryAnalyzer
from repo2txt.core.models import Config

# Traditional analysis
config = Config(enable_token_counting=True)
analyzer = RepositoryAnalyzer(config, theme="manhattan")
result = analyzer.analyze("/path/to/repo")

# AI-assisted analysis
config.ai_select = True
config.ai_query = "Select all Python files related to data processing"
result = analyzer.analyze("/path/to/repo")
```

## üêõ Troubleshooting

### Common Issues

**AI Selection Not Working**
- Verify LLM_API_KEY is set correctly
- Check LLM_BASE_URL for custom endpoints
- Use `--debug` to see system prompts and API calls

**Token Counting Errors**
- Use `--no-tokens` to disable if needed

**GitHub Rate Limiting**
- Set GITHUB_TOKEN environment variable
- Use personal access token with appropriate scopes

**Large Repository Performance**
- Use `--max-file-size` to limit individual files
- Set appropriate `--token-budget` for AI selection
- Consider analyzing specific subdirectories
- Models less performant than gpt-4.1-mini start to hallcinate when give large file tree's (> 250 files)

### Debug Mode

Enable comprehensive debugging:
```bash
repo2txt . --ai-select --debug
```

This shows:
- Message history sent to the LLM
- Tool calls and responses
- Token usage and budget tracking
- File selection reasoning

## ü§ù Contributing

We welcome contributions! Please see our contributing guidelines:

1. **Fork & Branch**: Create feature branches from `main`
2. **Test**: Ensure all tests pass with `pytest`
3. **Document**: Update README for user-facing changes
4. **Pull Request**: Submit with clear description of changes

### Development Setup
```bash
git clone https://github.com/your-username/repo2txt.git
cd repo2txt
uv sync --dev
source .venv/bin/activate
pytest  # Verify setup
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Original Concept**: [Doriandarko/RepoToTextForLLMs](https://github.com/Doriandarko/RepoToTextForLLMs)
- **Enhanced Architecture**:  Redesigned with imporve UX and added AI assistance.
